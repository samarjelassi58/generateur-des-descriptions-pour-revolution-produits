"""Scraper de produits Revolution Beauty.

Ce script extrait les informations d√©taill√©es de chaque page produit :
- Nom, description, prix
- Images (toutes les variantes)
- Breadcrumbs (fil d'Ariane)
- Notes et avis
- Ingr√©dients
- Variantes de produits (couleurs, tailles, etc.)

Le scraping est multi-thread√© pour des performances optimales et les donn√©es
sont sauvegard√©es progressivement dans un fichier CSV.

Usage:
    python scrapper.py

Entr√©e:
    data/urls.csv - Fichier contenant les URLs de produits (g√©n√©r√© par crawler.py)

Sortie:
    data/produits.csv - Fichier CSV contenant toutes les informations produits
"""

import csv
import requests
import time
import threading
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import json

# ================= CONFIG =================
# Fichier CSV contenant les URLs de produits √† scraper (g√©n√©r√© par crawler.py)
INPUT_URLS = "data/urls.csv"
# Fichier CSV de sortie contenant les donn√©es extraites
OUTPUT_PRODUCTS = "data/produits.csv"

# Nombre de threads simultan√©s pour le scraping parall√®le
MAX_WORKERS = 12
# Timeout en secondes pour les requ√™tes HTTP
TIMEOUT = 15
# D√©lai en secondes entre chaque requ√™te (pour √©viter de surcharger le serveur)
DELAY = 0.2

# En-t√™tes HTTP pour simuler un navigateur web
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Accept-Language": "fr-FR,fr;q=0.9",
}

# Lock pour synchroniser l'√©criture dans le fichier CSV entre threads
lock = threading.Lock()

# ================= HELPERS =================
def fetch(url):
    """R√©cup√®re le contenu HTML d'une URL.
    
    Ajoute un d√©lai avant la requ√™te pour respecter le serveur.
    
    Args:
        url (str): URL de la page √† r√©cup√©rer
        
    Returns:
        str|None: Contenu HTML de la page ou None en cas d'erreur
    """
    time.sleep(DELAY)
    try:
        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
        if r.status_code == 200:
            return r.text
    except Exception as e:
        print(f"‚ùå Erreur fetch URL {url}: {e}")
    return None


def is_product_page(soup):
    """V√©rifie si la page HTML correspond √† une page produit.
    
    Args:
        soup (BeautifulSoup): Objet BeautifulSoup de la page
        
    Returns:
        bool: True si c'est une page produit
    """
    return soup.select_one("div.l-pdp-content_inner") is not None


def text(soup, selector):
    """Extrait le texte d'un √©l√©ment HTML via un s√©lecteur CSS.
    
    Args:
        soup (BeautifulSoup): Objet BeautifulSoup de la page
        selector (str): S√©lecteur CSS de l'√©l√©ment
        
    Returns:
        str: Texte de l'√©l√©ment (nettoy√©) ou cha√Æne vide si non trouv√©
    """
    el = soup.select_one(selector)
    return el.get_text(strip=True) if el else ""


def attr(soup, selector, attr):
    """Extrait la valeur d'un attribut d'un √©l√©ment HTML.
    
    Args:
        soup (BeautifulSoup): Objet BeautifulSoup de la page
        selector (str): S√©lecteur CSS de l'√©l√©ment
        attr (str): Nom de l'attribut √† extraire
        
    Returns:
        str: Valeur de l'attribut ou cha√Æne vide si non trouv√©
    """
    el = soup.select_one(selector)
    return el.get(attr) if el else ""


# ================= EXTRACT =================
def extract_product(url, soup):
    """Extrait toutes les informations d'un produit depuis la page HTML.
    
    Parse le HTML de la page produit et extrait :
    - Informations de base (nom, description, prix)
    - Images (toutes les variantes disponibles)
    - Fil d'Ariane (breadcrumbs) pour la cat√©gorisation
    - Note et nombre d'avis clients
    - Ingr√©dients du produit
    - Variantes disponibles (couleurs, tailles, etc.)
    
    Args:
        url (str): URL de la page produit
        soup (BeautifulSoup): Objet BeautifulSoup de la page HTML
        
    Returns:
        dict: Dictionnaire contenant toutes les informations du produit
    """
    # --- Extraction des images ---
    # R√©cup√®re toutes les images du produit (galerie compl√®te)
    images = []
    for img in soup.select(".l-pdp-product_images img"):
        src = img.get("src")
        if src and src not in images:  # √âvite les doublons
            images.append(src)

    # --- Extraction du fil d'Ariane (breadcrumbs) ---
    # Cr√©e une cha√Æne "Cat√©gorie > Sous-cat√©gorie > Produit"
    breadcrumbs = " > ".join(
        a.get_text(strip=True)
        for a in soup.select("ul.b-breadcrumbs a")
    )

    # --- Extraction des ingr√©dients ---
    ingredients = text(soup, ".b-ingredients")

    # --- Extraction des variantes produit ---
    # R√©cup√®re toutes les variantes (couleurs, tailles, etc.)
    variantes = []
    for var in soup.select(".b-swatch_colors-item"):
        variantes.append({
            "name": var.get("data-js-display-value", ""),
            "id": var.get("data-js-variant-id", ""),
            "url": var.get("data-js-url", "")
        })
    # Convertit la liste en JSON pour stockage dans le CSV
    variantes_str = json.dumps(variantes, ensure_ascii=False)

    # --- Construction du dictionnaire produit ---
    return {
        "url": url,
        "name": text(soup, "h1.b-product_name"),
        "description": text(soup, "p.b-product_summary"),
        "price_sale": text(soup, "span.b-product_price-sales span.b-product_price-value"),
        "price_original": text(soup, "span.b-product_price-list span.b-product_price-value"),
        "discount": text(soup, "div.b-product_price-discount"),
        "breadcrumbs": breadcrumbs,
        "rating": attr(soup, ".yotpo-stars", "data-product-rating"),
        "reviews": text(soup, ".yotpo-bottomline a"),
        "images": "|".join(images),  # Images s√©par√©es par |
        "ingredients": ingredients,
        "variantes": variantes_str,  # JSON stringifi√©
    }


# ================= WORKER =================
def process_url(url, csv_file, fieldnames):
    """Traite une URL : r√©cup√®re, extrait et sauvegarde les donn√©es produit.
    
    Fonction ex√©cut√©e par chaque thread worker :
    1. R√©cup√®re le HTML de la page
    2. V√©rifie que c'est bien une page produit
    3. Extrait toutes les informations
    4. Sauvegarde imm√©diatement dans le CSV (thread-safe)
    
    Args:
        url (str): URL de la page produit √† scraper
        csv_file (str): Chemin du fichier CSV de sortie
        fieldnames (list): Liste des noms de colonnes du CSV
    """
    # R√©cup√®re le HTML de la page
    html = fetch(url)
    if not html:
        return

    # Parse le HTML avec BeautifulSoup
    soup = BeautifulSoup(html, "html.parser")

    # V√©rifie que c'est bien une page produit
    if not is_product_page(soup):
        return

    # Extrait toutes les informations du produit
    product = extract_product(url, soup)

    # Sauvegarde imm√©diate dans le CSV (thread-safe)
    with lock:
        with open(csv_file, "a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writerow(product)
        print("‚úÖ", product["name"])


# ================= MAIN =================
def main():
    """Fonction principale de scraping.
    
    Orchestre le processus de scraping multi-thread√© :
    1. Charge la liste des URLs depuis le fichier CSV
    2. Initialise le fichier CSV de sortie avec les en-t√™tes
    3. Lance le scraping parall√®le de toutes les URLs
    4. Sauvegarde progressive des r√©sultats (au fur et √† mesure)
    """
    # --- Chargement des URLs √† scraper ---
    with open(INPUT_URLS, newline="", encoding="utf-8") as f:
        urls = [row["url"] for row in csv.DictReader(f)]

    # --- D√©finition des colonnes du CSV de sortie ---
    fieldnames = [
        "url",           # URL de la page produit
        "name",          # Nom du produit
        "description",   # Description du produit
        "price_sale",    # Prix de vente actuel
        "price_original",# Prix original (avant r√©duction)
        "discount",      # R√©duction (pourcentage ou montant)
        "breadcrumbs",   # Fil d'Ariane (cat√©gorisation)
        "rating",        # Note moyenne du produit
        "reviews",       # Nombre d'avis clients
        "images",        # URLs des images (s√©par√©es par |)
        "ingredients",   # Liste des ingr√©dients
        "variantes"      # Variantes disponibles (JSON)
    ]

    # --- Cr√©ation du fichier CSV avec en-t√™tes ---
    with open(OUTPUT_PRODUCTS, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

    # --- Lancement du scraping multi-thread√© ---
    # Chaque URL est trait√©e en parall√®le par un thread
    # Les r√©sultats sont sauvegard√©s au fur et √† mesure
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_url, url, OUTPUT_PRODUCTS, fieldnames) for url in urls]
        # Attend que tous les threads terminent
        for _ in as_completed(futures):
            pass

    print("\nüéâ SCRAPING MULTITHREAD TERMIN√â")


if __name__ == "__main__":
    main()
